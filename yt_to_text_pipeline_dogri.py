# -*- coding: utf-8 -*-
"""YT_to_Text_pipeline_Dogri.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lvAjOsIMFRA05v9a1-GLC3NZyWkDz1T3

# Dogri CC-BY Corpus — Cheap API Path (Colab)

This notebook:
- Uses only cheap YouTube Data API calls (`channels.list`, `playlistItems.list`, `videos.list`) — **no `search.list`**.
- Scans seed **UC…** Dogri channels for **CC-BY** videos and saves a scan report.
- Builds a **legal, immediately usable** text corpus from *titles + descriptions* of CC-BY videos.
- (Optional) Adds fastText language ID tags.
- Saves outputs in `/content/dogri_out` and also as a downloadable ZIP.

**Notes**
- Keep **attribution** (channelTitle + videoId link) when using CC-BY text.
- Avoid downloading audio or non-owner captions via the API unless you have permission (YouTube ToS).

## Get a YouTube Data API key
1. Open **Google Cloud Console** → create/select a project.
2. **APIs & Services → Library** → enable **YouTube Data API v3**.
3. **APIs & Services → Credentials → + Create credentials → API key**.
4. (Recommended) **Restrict** the key (IP/app restrictions) once testing is stable.
"""

!pip -q install google-api-python-client pandas pyarrow regex tqdm python-dotenv fasttext-wheel

import os
from googleapiclient.discovery import build

os.environ["YT_API_KEY"] = "AIzaSyCOwOv8Ayxmajxj7OyiE1bXNLY3jwcRXjM"

API_KEY = os.getenv("YT_API_KEY")
assert API_KEY, "YouTube API key not set!"
youtube = build("youtube", "v3", developerKey=API_KEY, cache_discovery=False)
print("YouTube client ready.")

import os, unicodedata, time, json
import regex as re
import pandas as pd
from tqdm import tqdm
from datetime import datetime, timezone

OUT_DIR = "/content/dogri_out"
os.makedirs(OUT_DIR, exist_ok=True)

def now():
    return datetime.now(timezone.utc).isoformat(timespec="seconds")

# ====== Devanāgarī filter & normalization ======
ZW = r"[\u200B-\u200F\u202A-\u202E\u2060-\u206F]"
SP_MULT = re.compile(r"\s+", re.UNICODE)
DEVANAGARI_BLOCK = (0x0900, 0x097F)

def normalize_text(txt: str) -> str:
    if not txt:
        return ""
    t = unicodedata.normalize("NFKC", txt)
    t = re.sub(ZW, "", t)
    t = SP_MULT.sub(" ", t).strip()
    return t

def looks_devanagari(txt: str, threshold=0.4) -> bool:
    if not txt:
        return False
    total = sum(1 for ch in txt if ch.isalpha())
    if total == 0:
        return False
    dev = sum(1 for ch in txt if DEVANAGARI_BLOCK[0] <= ord(ch) <= DEVANAGARI_BLOCK[1])
    return (dev / total) >= threshold

# ====== Save helpers ======
def dump_df(df: pd.DataFrame, stem: str):
    csv_path = os.path.join(OUT_DIR, f"{stem}.csv")
    pq_path  = os.path.join(OUT_DIR, f"{stem}.parquet")
    df.to_csv(csv_path, index=False)
    try:
        df.to_parquet(pq_path, index=False)
    except Exception as e:
        print("Parquet write failed:", e)
    return csv_path, pq_path

# Optional: language ID tagging
MODEL_URL = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
LID_PATH = "/content/lid.176.ftz"

import urllib.request, os
if not os.path.exists(LID_PATH):
    print("Downloading fastText language-ID model (~8MB)…")
    urllib.request.urlretrieve(MODEL_URL, LID_PATH)
else:
    print("Language-ID model present.")

import fasttext
_ft = None
def load_ft():
    global _ft
    if _ft is None:
        _ft = fasttext.load_model(LID_PATH)
    return _ft

def langid(text: str):
    if not text:
        return ("unk", 0.0)
    ft = load_ft()
    labels, probs = ft.predict(text.replace("\n"," "), k=1)
    lab = labels[0].replace("__label__", "")
    return (lab, float(probs[0]))

# Add UC… Dogri channel IDs. (Open a channel page; copy the /channel/UC… from the URL.)


DOGRI_CHANNEL_IDS = [
    "UCFxjucTtvgi4gvTumv-seAA"
]

assert DOGRI_CHANNEL_IDS, "Add at least one UC… channel ID to DOGRI_CHANNEL_IDS."
print(f"[{now()}] Seed channels: {len(DOGRI_CHANNEL_IDS)}")

def uploads_playlist_id(channel_id: str):
    """channels.list (contentDetails) — returns uploads playlist id."""
    try:
        resp = youtube.channels().list(part="contentDetails", id=channel_id).execute()
        items = resp.get("items", [])
        if not items:
            return None
        return items[0]["contentDetails"]["relatedPlaylists"]["uploads"]
    except Exception:
        return None

def playlist_video_ids(playlist_id: str, max_pages=20, sleep=0.15):
    """playlistItems.list — collect video IDs (cheap)."""
    vids, page = [], None
    for _ in range(max_pages):
        data = youtube.playlistItems().list(
            part="contentDetails", playlistId=playlist_id,
            maxResults=50, pageToken=page
        ).execute()
        vids.extend(i["contentDetails"]["videoId"] for i in data.get("items", []))
        page = data.get("nextPageToken")
        if not page: break
        time.sleep(sleep)
    return vids

def videos_metadata(video_ids, sleep=0.15):
    """videos.list — batch metadata (status+snippet+contentDetails+statistics)."""
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = youtube.videos().list(
            part="status,snippet,contentDetails,statistics", id=",".join(batch)
        ).execute()
        for it in data.get("items", []):
            s = it.get("snippet", {})
            st = it.get("status", {})
            cd = it.get("contentDetails", {})
            rows.append({
                "videoId": it["id"],
                "channelTitle": s.get("channelTitle"),
                "title": s.get("title"),
                "description": s.get("description"),
                "publishedAt": s.get("publishedAt"),
                "defaultLanguage": s.get("defaultLanguage"),
                "defaultAudioLanguage": s.get("defaultAudioLanguage"),
                "duration": cd.get("duration"),
                "license": st.get("license"),  # "creativeCommon" or "youtube"
                "viewCount": it.get("statistics", {}).get("viewCount"),
            })
        time.sleep(sleep)
    return pd.DataFrame(rows)

def videos_meta_status(video_ids, sleep=0.15):
    """videos.list (status+snippet) for license check (lighter than full metadata)."""
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = youtube.videos().list(part="status,snippet", id=",".join(batch)).execute()
        for it in data.get("items", []):
            st = it.get("status", {})
            sn = it.get("snippet", {})
            rows.append({
                "videoId": it["id"],
                "license": st.get("license"),
                "title": sn.get("title"),
                "channelTitle": sn.get("channelTitle"),
            })
        time.sleep(sleep)
    return pd.DataFrame(rows)

def scan_channels_for_ccby(channel_ids, max_pages_per_channel=10):
    """Return a scan DataFrame + list of channelIds that have >=1 CC-BY video."""
    results = []
    good = []

    for ch in tqdm(channel_ids, desc="Scanning channels for CC-BY"):
        upl = uploads_playlist_id(ch)
        if not upl:
            results.append({
                "channelId": ch, "scanned": 0, "ccby_count": 0,
                "sample_ccby_urls": "—", "note": "No uploads playlist"
            })
            continue

        vids = playlist_video_ids(upl, max_pages=max_pages_per_channel)
        if not vids:
            results.append({
                "channelId": ch, "scanned": 0, "ccby_count": 0,
                "sample_ccby_urls": "—", "note": "No videos"
            })
            continue

        meta = videos_meta_status(vids)
        scanned = len(meta)
        ccby = meta[meta["license"] == "creativeCommon"]
        ccby_urls = [f"https://youtu.be/{v}" for v in ccby["videoId"].tolist()[:5]]
        count = len(ccby)

        if count > 0:
            good.append(ch)

        results.append({
            "channelId": ch,
            "scanned": scanned,
            "ccby_count": count,
            "sample_ccby_urls": ", ".join(ccby_urls) if ccby_urls else "—",
            "note": ""
        })

    df_scan = pd.DataFrame(results).sort_values(
        ["ccby_count","scanned"], ascending=[False, False]
    ).reset_index(drop=True)
    return df_scan, good

print(f"[{now()}] 🔎 Running CC-BY scan on seed channels…")
df_ccby_scan, GOOD_DOGRI_CHANNEL_IDS = scan_channels_for_ccby(
    DOGRI_CHANNEL_IDS, max_pages_per_channel=12
)

# Save scan report + good channels
scan_csv = os.path.join(OUT_DIR, "dogri_ccby_scan.csv")
df_ccby_scan.to_csv(scan_csv, index=False)

with open(os.path.join(OUT_DIR, "dogri_ccby_good_channels.json"), "w", encoding="utf-8") as f:
    json.dump(GOOD_DOGRI_CHANNEL_IDS, f, ensure_ascii=False, indent=2)

display(df_ccby_scan.head(20))
print(f"Good channels with ≥1 CC-BY video: {len(GOOD_DOGRI_CHANNEL_IDS)}")
GOOD_DOGRI_CHANNEL_IDS[:10]

print(f"[{now()}] 🎬 Expanding channels for CORPUS…")

channel_source = GOOD_DOGRI_CHANNEL_IDS if len(GOOD_DOGRI_CHANNEL_IDS) > 0 else DOGRI_CHANNEL_IDS
print("Using channels:", len(channel_source), "(filtered good)" if channel_source is GOOD_DOGRI_CHANNEL_IDS else "(all seeds)")

all_meta = []
for ch in tqdm(channel_source, desc="Channels → corpus"):
    try:
        upl = uploads_playlist_id(ch)
        if not upl:
            print("No uploads playlist for:", ch)
            continue
        vids = playlist_video_ids(upl, max_pages=30)
        if not vids:
            continue
        meta = videos_metadata(vids)
        meta["channelId"] = ch
        all_meta.append(meta)
    except Exception as e:
        print("Error expanding", ch, ":", repr(e))

if not all_meta:
    raise SystemExit("No metadata collected. Add more seed channels or check quota.")

meta_df = pd.concat(all_meta, ignore_index=True).drop_duplicates(subset=["videoId"])
dump_df(meta_df, "videos_with_meta_raw")
print(f"[{now()}] Meta rows collected: {len(meta_df)}")

# ---- Keep CC-BY only ----
ccby_df = meta_df[meta_df["license"] == "creativeCommon"].copy()
print(f"[{now()}] CC-BY videos: {len(ccby_df)}")
dump_df(ccby_df, "videos_with_meta_ccby")

# ---- Extract usable text: title + description ----
ccby_df["title_n"] = ccby_df["title"].fillna("").map(normalize_text)
ccby_df["desc_n"]  = ccby_df["description"].fillna("").map(normalize_text)
ccby_df["text_concat"] = (ccby_df["title_n"] + " | " + ccby_df["desc_n"]).str.strip(" |")

# Prefer Devanāgarī-heavy (Dogri often in Devanāgarī; tune threshold as needed)
ccby_df["dev_ok"] = ccby_df["text_concat"].map(lambda t: looks_devanagari(t, threshold=0.4))
keep_df = ccby_df[ccby_df["dev_ok"] & (ccby_df["text_concat"].str.len() > 10)].copy()

# Dedupe on normalized text
keep_df["dedupe_key"] = keep_df["text_concat"].str.lower().str.replace(r"\s+", " ", regex=True)
keep_df = keep_df.drop_duplicates(subset=["dedupe_key"])

# Optional: language-ID tags
try:
    labs, probs = [], []
    for txt in keep_df["text_concat"].fillna(""):
        lab, p = langid(txt)
        labs.append(lab); probs.append(p)
    keep_df["lang_pred"] = labs
    keep_df["lang_conf"] = probs
except Exception as e:
    print("Language-ID skipped:", e)

csv_path, pq_path = dump_df(keep_df, "dogri_ccby_text_titles_desc")
print(f"[{now()}] ✅ Final text rows: {len(keep_df)}")
keep_df.head(10)

import shutil
from google.colab import files

zip_path = "/content/dogri_out.zip"
shutil.make_archive("/content/dogri_out", "zip", "/content/dogri_out")
print("Zipped to:", zip_path)
files.download(zip_path)