# -*- coding: utf-8 -*-
"""SarvamAI_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gCE-zU0bptSh4TOK6H4R2XS5TGW8NnGM
"""

!pip install -q transformers torch accelerate bitsandbytes

import torch
print(f"✓ GPU Available: {torch.cuda.is_available()}")
print(f"✓ GPU Device: {torch.cuda.get_device_name(0)}")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "sarvamai/sarvam-translate"

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Loading model... (this may take 1-2 minutes)")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    low_cpu_mem_usage=True
)

print("✓ Model loaded successfully!")
print(f"✓ Model device: {model.device}")
print(f"✓ Model dtype: {model.dtype}")

"""   
    Translate text to target language.
    
    Args:
        text: Text to translate
        target_language: Target language (e.g., "Hindi", "Tamil", "Bengali")
        temperature: Lower = more deterministic (default: 0.01)
    
    Returns:
        Translated text

"""

@torch.inference_mode()
def translate_text(text: str, target_language: str = "Marathi", temperature: float = 0.01) -> str:

    messages = [
        {"role": "system", "content": f"Translate the text below to {target_language}."},
        {"role": "user", "content": text}
    ]

    # Format prompt
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    inputs = tokenizer([prompt], return_tensors="pt").to(model.device)

    # Generate translation
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        do_sample=True,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    # Decode only the generated part
    output_ids = outputs[0][len(inputs.input_ids[0]):]
    translation = tokenizer.decode(output_ids, skip_special_tokens=True)

    return translation.strip()

# Test translation
test_text = "Trees are very important for life on Earth. They give us oxygen to breathe and help clean the air by taking in harmful gases. Trees provide food, fruits, and wood for people. They give shade and make our surroundings cool. Forests are homes for many animals, birds, and insects, which keeps nature balanced. Being near trees makes people feel calm and happy. Planting new trees and taking care of existing ones is very important. By protecting trees, we can save the environment, fight climate change, and make the Earth a better place for everyone."
result = translate_text(test_text, "Marathi")

print(f"Input: {test_text}")
print(f"Translation: {result}")

"""***BATCH TESTING***"""

def batch_translate(texts: list, target_language: str = "Hindi", show_progress: bool = True) -> list:
    """Translate multiple texts with progress tracking."""
    translations = []

    for i, text in enumerate(texts, 1):
        if show_progress:
            print(f"Translating {i}/{len(texts)}...", end='\r')

        translation = translate_text(text, target_language)
        translations.append(translation)

    if show_progress:
        print(f"✓ Completed {len(texts)} translations!")

    return translations

# Example usage
texts = [
    "Hello, how are you?",
    "Good morning!",
    "Thank you very much.",
    "What is your name?",
    "Have a great day!"
]

results = batch_translate(texts, "Hindi")

print("\n" + "="*60)
for original, translated in zip(texts, results):
    print(f"EN: {original}")
    print(f"HI: {translated}")
    print("-"*60)